{"type":"data","nodes":[{"type":"data","data":[{"settings":1},{"id":2,"uid":3,"url":3,"type":4,"href":5,"tags":6,"first_publication_date":7,"last_publication_date":8,"slugs":9,"linked_documents":10,"lang":11,"alternate_languages":12,"data":13},"Zh-f0BUAANmXr9IL",null,"settings","https://voice-sense.cdn.prismic.io/api/v2/documents/search?ref=Zj3dyRIAACIA9k2e&q=%5B%5B%3Ad+%3D+at%28document.id%2C+%22Zh-f0BUAANmXr9IL%22%29+%5D%5D",[],"2024-04-17T10:09:33+0000","2024-05-10T04:23:58+0000",[],[],"en-us",[],{"site_title":14,"meta_description":15,"og_image":16,"navigation":25},"Voice Sense","Voice Sense allows you to detect Emotions through vocal expressions",{"dimensions":17,"alt":3,"copyright":3,"url":19,"id":20,"edit":21},{"width":18,"height":18},732,"https://images.prismic.io/voice-sense/Zh-fnUaI3ufuUP6q_voice-sense.png?auto=format,compress","Zh-fnUaI3ufuUP6q",{"x":22,"y":22,"zoom":23,"background":24},0,1,"transparent",[26,39,48],{"link":27,"label":38,"cta_button":37},{"id":28,"type":29,"tags":30,"lang":11,"slug":31,"first_publication_date":32,"last_publication_date":33,"uid":34,"url":35,"link_type":36,"isBroken":37},"ZBHedhAAAKdKz5Kp","page",[],"-","2024-04-25T07:54:12+0000","2024-04-25T10:45:01+0000","about-us-page","/about-us-page","Document",false,"ABOUT US",{"link":40,"label":47,"cta_button":37},{"id":41,"type":29,"tags":42,"lang":11,"slug":31,"first_publication_date":43,"last_publication_date":44,"uid":45,"url":46,"link_type":36,"isBroken":37},"Zj2g9RIAANvM9eg-",[],"2024-05-10T04:22:23+0000","2024-05-10T04:23:27+0000","documentation","/documentation","DOCUMENTATION",{"link":49,"label":51,"cta_button":52},{"link_type":50},"Any","TRY NOW",true],"uses":{}},{"type":"data","data":[{"page":1,"title":18,"meta_description":129,"meta_title":128,"meta_image":134},{"id":2,"uid":3,"url":4,"type":5,"href":6,"tags":7,"first_publication_date":8,"last_publication_date":9,"slugs":10,"linked_documents":11,"lang":12,"alternate_languages":13,"data":14},"ZBHc0BAAACoAz4-X","home","/","page","https://voice-sense.cdn.prismic.io/api/v2/documents/search?ref=Zj3dyRIAACIA9k2e&q=%5B%5B%3Ad+%3D+at%28document.id%2C+%22ZBHc0BAAACoAz4-X%22%29+%5D%5D",[],"2024-04-16T17:54:56+0000","2024-05-10T08:41:45+0000",[],[],"en-us",[],{"title":15,"slices":21,"meta_title":128,"meta_description":129,"meta_image":130},[16],{"type":17,"text":18,"spans":19,"direction":20},"heading1","Voice Sense",[],"ltr",[22,53],{"variation":23,"version":24,"items":25,"primary":26,"id":51,"slice_type":52,"slice_label":44},"default","initial",[],{"heading":27,"body":31,"button_link":36,"button_lable":38,"hero_image":39},[28],{"type":17,"text":29,"spans":30,"direction":20},"Unlock the power of emotion with AI Voice Analytics.",[],[32],{"type":33,"text":34,"spans":35,"direction":20},"paragraph","Channel your inner Emotions by using our Cutting Edge Technology that detects emotion from your \" VOICE \"  using Advance Artificial Intelligence ",[],{"link_type":37},"Any","TRY NOW",{"dimensions":40,"alt":43,"copyright":44,"url":45,"id":46,"edit":47},{"width":41,"height":42},4368,2852,"man wearing black long-sleeved shirt",null,"https://images.unsplash.com/photo-1494459158735-82f8feb14abb?crop=entropy&cs=srgb&fm=jpg&ixid=M3wzMzc0NjN8MHwxfHNlYXJjaHw3fHxzdHJlc3N8ZW58MHx8fHwxNzEzNjEwMDk3fDA&ixlib=rb-4.0.3&q=85?auto=compress,format","wuo8KnyCm4I",{"x":48,"y":48,"zoom":49,"background":50},0,1,"transparent","hero$72b23d93-76c4-4b19-9cc9-f6d07e1c8c47","hero",{"variation":23,"version":24,"items":54,"primary":109,"id":126,"slice_type":127,"slice_label":44},[55,74,92],{"bentotitle":56,"bentobody":61,"bentoimage":65,"iswide":73},[57],{"type":58,"text":59,"spans":60,"direction":20},"heading3","Convolution Neural Network",[],[62],{"type":33,"text":63,"spans":64,"direction":20},"CNNs can extract features from voice data by analyzing spectrograms",[],{"dimensions":66,"alt":69,"copyright":44,"url":70,"id":71,"edit":72},{"width":67,"height":68},730,580,"CNN Reference image","https://images.prismic.io/voice-sense/ZiQE5PPdc1huKp4-_CNN.png?auto=format,compress","ZiQE5PPdc1huKp4-",{"x":48,"y":48,"zoom":49,"background":50},false,{"bentotitle":75,"bentobody":79,"bentoimage":83,"iswide":91},[76],{"type":58,"text":77,"spans":78,"direction":20},"Reccurent Neural Network",[],[80],{"type":33,"text":81,"spans":82,"direction":20},"With their ability to retain memory of previous inputs, RNNs can effectively capture temporal dependencies in audio signals, making them suitable for tasks such as speech recognition and natural language processing",[],{"dimensions":84,"alt":87,"copyright":44,"url":88,"id":89,"edit":90},{"width":85,"height":86},1211,314,"RNN Reference Image","https://images.prismic.io/voice-sense/ZiQFY_Pdc1huKp5E_Rnn.png?auto=format,compress","ZiQFY_Pdc1huKp5E",{"x":48,"y":48,"zoom":49,"background":50},true,{"bentotitle":93,"bentobody":97,"bentoimage":101,"iswide":91},[94],{"type":58,"text":95,"spans":96,"direction":20},"Audio Data Samples",[],[98],{"type":33,"text":99,"spans":100,"direction":20},"more than 14000 audio data samples with 7 different emotions that are used to Train the AI model to increase Prediction Accuracy ",[],{"dimensions":102,"alt":105,"copyright":44,"url":106,"id":107,"edit":108},{"width":103,"height":104},1402,326,"Emotions Percieved","https://images.prismic.io/voice-sense/ZiQNDPPdc1huKp5y_emotions_emoji.png?auto=format,compress","ZiQNDPPdc1huKp5y",{"x":48,"y":48,"zoom":49,"background":50},{"heading":110,"body":122},[111],{"type":112,"text":113,"spans":114,"direction":20},"heading2","The Power of \nFeature Extraction",[115,119],{"start":116,"end":117,"type":118},3,4,"em",{"start":120,"end":121,"type":118},14,32,[123],{"type":33,"text":124,"spans":125,"direction":20},"Introducing New approach for Emotion Detection Dynamically using AI",[],"bento_box$71165df4-0dee-4f17-b600-585947312c30","bento_box","Homepage","The homepage of your new website.",{"dimensions":131,"alt":44,"copyright":44,"url":134,"id":135,"edit":136},{"width":132,"height":133},2400,1260,"https://images.prismic.io/nextjs-starter-prismic-minimal/1ef1c411-4d12-4f07-af9e-a510683d8be7_cody-silver-eKB9CoaIUDo-unsplash.jpg?auto=compress,format&rect=0,7,1920,1008&w=2400&h=1260","ZKxRnhEAACIAcpFB",{"x":48,"y":137,"zoom":138,"background":139},-9,1.25,"#fff"],"uses":{}}]}
